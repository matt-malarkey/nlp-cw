{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\r\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\r\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in ./.virtualenv/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.3)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\r\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\r\n",
      "Requirement already satisfied: pathy in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.56.2)\r\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\r\n",
      "Requirement already satisfied: setuptools in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (53.0.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in ./.virtualenv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./.virtualenv/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.virtualenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.virtualenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./.virtualenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.virtualenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\r\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in ./.virtualenv/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./.virtualenv/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in ./.virtualenv/lib/python3.8/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\r\n",
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Matt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import nltk\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "ner = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('dev.csv')\n",
    "train_proportion = 0.8\n",
    "\n",
    "original_words = []\n",
    "for i in range(len(train_df)):\n",
    "  original_words.append(re.search(r'<(.*)/>', train_df['original'][i]).group(1))\n",
    "  train_df.at[i, 'original'] = re.sub(r'<.*/>', train_df['edit'][i], train_df['original'][i])\n",
    "train_df['originalWord'] = original_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class Task1DatasetNN(Dataset):\n",
    "\n",
    "  def __init__(self, train_data, labels, max_len, word2idx, entity2idx, stopwords=nltk.corpus.stopwords.words('english')):\n",
    "    self.x_train = train_data\n",
    "    self.y_train = labels\n",
    "    self.stopwords = stopwords\n",
    "    self.max_len = max_len\n",
    "    self.word2idx = word2idx\n",
    "    self.entity2idx = entity2idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    headline = self.x_train[item]\n",
    "\n",
    "    # Remove punctuation\n",
    "    headline = re.sub(r'[^\\w\\s]', '', headline)\n",
    "\n",
    "    # Map headlines to list of entity IDs\n",
    "    # entities = [self.entity2idx[str(e)] for e in ner(headline).ents if str(e) in self.entity2idx]\n",
    "\n",
    "    # Tokenize, lowercase, remove stop words\n",
    "    headline = [w.lower() for w in headline.split() if w not in self.stopwords and len(w)]\n",
    "\n",
    "    # Get headline vector\n",
    "    vectorized_headline = [self.word2idx[tok] for tok in headline if tok in self.word2idx]\n",
    "    headline_tensor = torch.zeros((self.max_len,)).long()\n",
    "    headline_tensor[:len(vectorized_headline)] = torch.LongTensor(vectorized_headline)\n",
    "\n",
    "    target_tensor = torch.FloatTensor([self.y_train[item]])\n",
    "    extra_features = []\n",
    "\n",
    "    return {\n",
    "      'encoding': headline_tensor,\n",
    "      'extra_features': torch.tensor(extra_features),\n",
    "      'target': target_tensor\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "TOKEN_PATTERN = re.compile(\"\\w+\")\n",
    "def tokenize(sentence):\n",
    "  return TOKEN_PATTERN.findall(sentence)\n",
    "\n",
    "# Remove angle brackets and lowercase\n",
    "def preprocess(word):\n",
    "  if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
    "    word = word[1:-2]\n",
    "  return word.lower()\n",
    "\n",
    "# Create vocab of all words\n",
    "def create_vocab(data):\n",
    "  # Let us put the tokenized corpus in a list\n",
    "  tokenized_corpus = [[preprocess(t) for t in tokenize(s)] for s in data]\n",
    "\n",
    "  # Create single list of all vocabulary\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "      if token not in vocabulary:\n",
    "        vocabulary.append(token)\n",
    "\n",
    "  return vocabulary, tokenized_corpus\n",
    "\n",
    "# TODO: need to tie this in with NER\n",
    "def build_word2idx(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "      if token not in vocabulary:\n",
    "        vocabulary.append(token)\n",
    "  w2i = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "  w2i['<pad>'] = 0\n",
    "  return w2i\n",
    "\n",
    "# Get NER lookup table from training data\n",
    "def build_entity2idx(corpus):\n",
    "  entities = {}\n",
    "  idx = 0\n",
    "  for s in corpus:\n",
    "    doc = ner(s)\n",
    "    for e in doc.ents:\n",
    "      e = str(e)\n",
    "      if e not in entities:\n",
    "        entities[e] = idx\n",
    "        idx+=1\n",
    "  return entities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# e2i = get_entity2idx(corpus)\n",
    "# print(e2i)\n",
    "# print(type(e2i['Trump']))\n",
    "# efs = get_entity_features(corpus, e2i)\n",
    "# print(efs[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):\n",
    "    super(FFNN, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "    self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    self.relu2 = nn.ReLU()\n",
    "\n",
    "  # Average sentence embedding (ignoring padding)\n",
    "  # Maps embedded : (batch_size, max_sent_len, embedding_dim) => (batch_size, embeddind_dim)\n",
    "  def average_embedding(self, embedded):\n",
    "    # Want to calculate the avg embedding across all words\n",
    "    # This is the max_sent_len dimension so dim=1\n",
    "    total_emb = torch.sum(embedded, dim=1)\n",
    "\n",
    "    # Count number of non-zero embeddings to average over\n",
    "    non_zero_embs = torch.sum(embedded != 0, dim=[1])\n",
    "    avg_emb = total_emb / non_zero_embs\n",
    "\n",
    "    return avg_emb\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batch_size, max_sent_len)\n",
    "    embedded = self.embedding(x)\n",
    "\n",
    "    # embedding has shape (batch_size, max_sent_len, embedding_dim)\n",
    "    # Compute the average embeddings of shape (batch_size, embedding_dim)\n",
    "    averaged = self.average_embedding(embedded)\n",
    "\n",
    "    # TODO: extra features\n",
    "\n",
    "    out = self.fc1(averaged)\n",
    "    out = self.relu1(out)\n",
    "    out = self.fc2(out)\n",
    "    # out = self.relu2(out)\n",
    "    return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LRATE = 0.1\n",
    "EMBEDDING_DIM = 20 # ~4th root of vocab size\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Set training and test data\n",
    "training_data = train_df['original']\n",
    "training_labels = train_df['meanGrade']\n",
    "test_data = test_df['original']\n",
    "\n",
    "_vocab, tokenized_corpus = create_vocab(training_data)\n",
    "max_headline_len = max([len([w for w in s.split()]) for s in training_data])\n",
    "w2i = build_word2idx(tokenized_corpus)\n",
    "# e2i = build_entity2idx(training_data)\n",
    "e2i = {}\n",
    "\n",
    "train_and_dev = Task1DatasetNN(training_data, training_labels, max_headline_len, w2i, e2i)\n",
    "\n",
    "# TODO: what processing should be done on both train and dev and what should be done on just train???\n",
    "# Split training data into train and dev sets\n",
    "train_examples = round(len(training_data)*train_proportion)\n",
    "dev_examples = len(training_data) - train_examples\n",
    "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))\n",
    "\n",
    "# train_sent_tensor, train_label_tensor\n",
    "i2w = {v:k for k,v in w2i.items()}\n",
    "\n",
    "# for a, d in zip(training_data, train_and_dev):\n",
    "#   d = d['encoding'].tolist()\n",
    "#   dec = [i2w[i] for i in d]\n",
    "#   print(a)\n",
    "#   print(dec)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (embedding): Embedding(7265, 20, padding_idx=0)\n",
      "  (fc1): Linear(in_features=20, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "| Epoch: 00 | Train Loss: 0.246\n",
      "| Epoch: 01 | Train Loss: 0.219\n",
      "| Epoch: 02 | Train Loss: 0.457\n",
      "| Epoch: 03 | Train Loss: 0.537\n",
      "| Epoch: 04 | Train Loss: 0.451\n",
      "| Epoch: 05 | Train Loss: 0.240\n",
      "| Epoch: 06 | Train Loss: 0.418\n",
      "| Epoch: 07 | Train Loss: 0.493\n",
      "| Epoch: 08 | Train Loss: 0.415\n",
      "| Epoch: 09 | Train Loss: 0.337\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 50\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Construct the model\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(w2i), OUTPUT_DIM)\n",
    "print(model)\n",
    "\n",
    "# we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "\n",
    "# we use the binary cross-entropy loss with sigmoid (applied to logits)\n",
    "# Recall that we did not apply any activation to our output layer, hence we need\n",
    "# to make our outputs look like probabilities.\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Start training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "  # good practice to include even if do not use here\n",
    "  model.train()\n",
    "\n",
    "  for batch in train_loader:\n",
    "\n",
    "    encoding = batch['encoding'].to(device)\n",
    "    target = batch['target'].to(device).squeeze(1)\n",
    "    # extra_features = batch['extra_features'].to(device)\n",
    "\n",
    "    # we zero the gradients as they are not removed automatically\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
    "    # and we need to remove the dimension of size 1\n",
    "    predictions = model(encoding).squeeze(1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(predictions, target)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # calculate the gradient of each parameter\n",
    "    loss.backward()\n",
    "\n",
    "    # update the parameters using the gradients and optimizer algorithm\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c991d495",
   "language": "python",
   "display_name": "PyCharm (nlp-cw)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}