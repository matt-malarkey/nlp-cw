{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "task_1_main-6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvyemfDlDmu"
      },
      "source": [
        "# NLP CW Task 1\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWFk-kelDoA"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "import torchtext\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binned_statistic\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk6g6430Lsen"
      },
      "source": [
        "# Get pretrained GloVe embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJCEYZNOleKm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Install packages that Colab does not provide automatically\n",
        "!pip -q install transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, BertPreTrainedModel, DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download spacy model\n",
        "!spacy download en_core_web_sm -q\n",
        "ner = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Get test and train data files\n",
        "if not os.path.exists('dev.csv'):\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/dev.csv\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/train.csv\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/competition_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09jt8VRlDoM"
      },
      "source": [
        "# Set random seed and device\n",
        "SEED = 1\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqhlzLl6lDoO"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('dev.csv')\n",
        "comp_df = pd.read_csv('competition_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP5Yz343ddbL"
      },
      "source": [
        "# Build columns in dataset for edited headline and original word\n",
        "def build_edit_cols(dataset):\n",
        "  edited = []\n",
        "  original_words = []\n",
        "  for i in range(len(dataset)):\n",
        "    original_words.append(re.search(r'<(.*)/>', dataset['original'][i]).group(1))\n",
        "    edited.append(re.sub(r'<.*/>', dataset['edit'][i], dataset['original'][i]))\n",
        "  dataset['edited'] = edited\n",
        "  dataset['originalWord'] = original_words\n",
        "\n",
        "build_edit_cols(train_df)\n",
        "build_edit_cols(test_df)\n",
        "build_edit_cols(comp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zZIRguoZYQ-"
      },
      "source": [
        "# Approach 1\n",
        "Using pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inHh1HElZYQ_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, optimizer):\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  performance_stats = []\n",
        "\n",
        "  for epoch in range(1, number_epoch+1):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0  # Observations used for training so far\n",
        "\n",
        "    for batch in train_iter:\n",
        "      ids = batch['ids'].to(DEVICE).squeeze()\n",
        "      mask = batch['mask'].to(DEVICE).squeeze()\n",
        "      target = batch['target'].to(DEVICE, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "    performance_stats.append((valid_loss, valid_mse**0.5))\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "    Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
        "\n",
        "  return performance_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXeDgHmlDob"
      },
      "source": [
        "# Evaluate performance\n",
        "def eval(data_iter, model):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      ids = batch['ids'].to(DEVICE).squeeze()\n",
        "      mask = batch['mask'].to(DEVICE).squeeze()\n",
        "      target = batch['target'].to(DEVICE, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # We get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_22fHHElDog"
      },
      "source": [
        "# Get SSE and MSE\n",
        "def model_performance(output, target, print_output=False):\n",
        "  sq_error = (output - target)**2\n",
        "  sse = np.sum(sq_error)\n",
        "  mse = np.mean(sq_error)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  if print_output:\n",
        "    print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "  return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyew1QfTSuzM"
      },
      "source": [
        "# Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhQ12OmSzD6"
      },
      "source": [
        "# Basic Stats\n",
        "num_headlines = len(train_df)\n",
        "max_len = max(train_df['original'].apply(len))\n",
        "avg_score = train_df['meanGrade'].mean()\n",
        "print(f'Number of Headlines: {num_headlines}, Max Headline Length: {max_len}, Avg Score: {avg_score:.2f}')\n",
        "\n",
        "# Most common words\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "most_common_words = Counter(\" \".join(train_df['original']).split()).most_common(100)\n",
        "stopwords.append(string.punctuation)\n",
        "most_common_words = [(word, count) for (word, count) in most_common_words if word not in stopwords and word.isalpha()][:5]\n",
        "print(f'Most common words: {most_common_words}')\n",
        "\n",
        "# Analysis of Trump Headlines\n",
        "trump_headlines = train_df[train_df['original'].str.lower().str.contains('trump')]\n",
        "num_headlines_with_trump = len(trump_headlines)\n",
        "trump_avg_score = trump_headlines['meanGrade'].mean()\n",
        "prob_headline_contains_trump = num_headlines_with_trump / num_headlines\n",
        "trump_value_counts = trump_headlines['meanGrade'].value_counts(bins=3)\n",
        "trump_proportion_gt_one = (trump_value_counts[2] + trump_value_counts[3]) * 100 / num_headlines_with_trump\n",
        "print(f'Headlines with Trump: {prob_headline_contains_trump * 100:.2f}%, Avg Score: {trump_avg_score:.2f}')\n",
        "print(f'Proportion of Trump Headlines with score > 1: {trump_proportion_gt_one:.2f}%')\n",
        "\n",
        "# Analysis of word similarity and length of headlines\n",
        "semantic_similarity_data = []\n",
        "sentence_similarity_data = []\n",
        "levenshtein_similarity_data = []\n",
        "headline_length_data = []\n",
        "\n",
        "for row in train_df.itertuples():\n",
        "  all_words = row.original.split(' ')\n",
        "  headline_length_data.append((len(all_words), row.meanGrade))\n",
        "  sentence_vec = torch.zeros((1,50))\n",
        "  original_word_vec = glove[row.originalWord.lower()].unsqueeze(0)\n",
        "  edited_word_vec = glove[row.edit.lower()].unsqueeze(0)\n",
        "\n",
        "  for i, word in enumerate(all_words):\n",
        "    if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "      word = word[1:-2].lower()\n",
        "    sentence_vec += glove[word.lower()].unsqueeze(0)\n",
        "\n",
        "  semantic_distance = torch.cosine_similarity(original_word_vec, edited_word_vec).item()\n",
        "  levenshtein_distance = nltk.edit_distance(row.originalWord.lower(), row.edit.lower())\n",
        "  semantic_similarity_data.append((semantic_distance, row.meanGrade))\n",
        "  levenshtein_similarity_data.append((levenshtein_distance, row.meanGrade))\n",
        "  sentence_semantic_distance = torch.cosine_similarity(sentence_vec, edited_word_vec)\n",
        "  sentence_similarity_data.append((sentence_semantic_distance, row.meanGrade))\n",
        "\n",
        "\n",
        "# Analysis of # of clause separators\n",
        "separators_data = []\n",
        "for i in range(len(train_df['original'])):\n",
        "  matches = re.findall(\"[;,:(...)]\", train_df['original'][i])\n",
        "  separators_data.append((len(matches), train_df['meanGrade'][i]))\n",
        "\n",
        "# Plot word similarity vs meanGrade\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 3, 1)\n",
        "mean_stat_similarity = binned_statistic(*zip(*semantic_similarity_data), bins=6, range=(-0.5, 1.0))\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Cosine Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Semantic similarity vs meanGrade')\n",
        "\n",
        "# Levenshtein distance vs meanGrade\n",
        "plt.subplot(2, 3, 2)\n",
        "mean_stat_similarity = binned_statistic(*zip(*levenshtein_similarity_data), bins=6)\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Levenshtein Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Word spelling similarity vs meanGrade')\n",
        "\n",
        "# Semantic similarity of word to headline vs meanGrade\n",
        "plt.subplot(2, 3, 3)\n",
        "mean_stat_similarity = binned_statistic(*zip(*sentence_similarity_data), bins=6)\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Cosine Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Semantic similarity of word to headline vs meanGrade')\n",
        "\n",
        "# Plot clause separator count vs meanGrade\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot()\n",
        "mean_stat_seps = binned_statistic(*zip(*separators_data), bins=6)\n",
        "plt.plot(mean_stat_seps.bin_edges[:-1], mean_stat_seps.statistic)\n",
        "plt.xlabel('Number of Clause Separators')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Clause Separator Count vs meanGrade')\n",
        "\n",
        "# Plot headline length vs meanGrade\n",
        "plt.subplot(2, 3, 5)\n",
        "mean_stat_headline_len = binned_statistic(*zip(*headline_length_data), bins=30, range=(0, 30))\n",
        "plt.plot(mean_stat_headline_len.bin_edges[:-1], mean_stat_headline_len.statistic)\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Headline Length vs meanGrade')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cenE48y9leKo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2spyhNrBleKo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Pattern to match words and multiword tags\n",
        "# See https://regex101.com/r/a2cWZL/1 for example\n",
        "# TOKEN_PATTERN = re.compile(\"[A-Za-z#]+|<[A-Za-z#]+\\s[A-Za-z]+\\/>\")\n",
        "\n",
        "# Patten to match only words\n",
        "TOKEN_PATTERN = re.compile(\"\\w+\")\n",
        "CLAUSE_SEPS = re.compile(\"[;,:(...)]\")\n",
        "\n",
        "# TODO: processing of:\n",
        "# - hashtags,\n",
        "# - words with a \"-\" in between them\n",
        "# - punctuation\n",
        "# - entity names\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return TOKEN_PATTERN.findall(sentence)\n",
        "\n",
        "# Remove angle brackets and lowercase\n",
        "def preprocess(word):\n",
        "  if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "    word = word[1:-2]\n",
        "  return word.lower()\n",
        "\n",
        "def create_vocab(data):\n",
        "  \"\"\"\n",
        "  Creating a corpus of all the tokens used\n",
        "  \"\"\"\n",
        "  # Let us put the tokenized corpus in a list\n",
        "  tokenized_corpus = [[preprocess(t) for t in tokenize(s)] for s in data]\n",
        "\n",
        "  # Create single list of all vocabulary\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "  return vocabulary, tokenized_corpus\n",
        "\n",
        "# Extra features:\n",
        "# [0] - binary if 'Trump' in headline\n",
        "# [1] - length of headline\n",
        "# [2] - number of clauses in a headline\n",
        "# [3] - semantic distance between original and edit word glove embedding\n",
        "# [4] - semantic distance between edit word and total original headline glove embedding\n",
        "# [5] - edit distance between edit word and original word\n",
        "EXTRA_FEATURES = 6\n",
        "def extract_features(raw_headline, original_word, edit_word):\n",
        "  sentence_vec = torch.zeros((1, 50))\n",
        "  original_word_vec = glove[original_word.lower()].unsqueeze(0)\n",
        "  edited_word_vec = glove[edit_word.lower()].unsqueeze(0)\n",
        "\n",
        "  for i, word in enumerate(raw_headline.split(' ')):\n",
        "    if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "      word = word[1:-2].lower()\n",
        "    sentence_vec += glove[word.lower()].unsqueeze(0)\n",
        "\n",
        "  semantic_distance = torch.cosine_similarity(original_word_vec, edited_word_vec).item()\n",
        "  levenshtein_distance = nltk.edit_distance(original_word.lower(), edit_word.lower())\n",
        "  sentence_semantic_distance = torch.cosine_similarity(sentence_vec, edited_word_vec)\n",
        "\n",
        "  return [\n",
        "    int('Trump' in raw_headline),\n",
        "    len(raw_headline),\n",
        "    len(CLAUSE_SEPS.findall(raw_headline)) + 1,\n",
        "    semantic_distance,\n",
        "    sentence_semantic_distance,\n",
        "    levenshtein_distance\n",
        "  ]\n",
        "\n",
        "EXTRA_FEATURES_TWO = 3\n",
        "def extract_features_two(raw_headline):\n",
        "  return [\n",
        "    int('Trump' in raw_headline),\n",
        "    len(raw_headline),\n",
        "    len(CLAUSE_SEPS.findall(raw_headline)) + 1\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoC2hQrNleKp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Inherits from Dataset class so the DataLoader can use it\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, tokenizer, train_data, labels):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    # Preprocess headline before embedding\n",
        "    headline = self.x_train[0][item]\n",
        "    original_word = self.x_train[1][item]\n",
        "    edit_word = self.x_train[2][item]\n",
        "    # processed_headline = ' '.join([preprocess(t) for t in tokenize(headline)])\n",
        "    processed_headline = [preprocess(t) for t in tokenize(headline)]\n",
        "\n",
        "    target = self.y_train[item]\n",
        "\n",
        "    # Use is_split_into_words=True to allow for our own tokenizing\n",
        "    # - stops encode_plus splitting up unknown words into multiple parts\n",
        "    encoding = self.tokenizer.encode_plus(processed_headline, return_tensors='pt',\n",
        "                                          padding='max_length', truncation=True,\n",
        "                                          max_length=128, pad_to_max_length=True,\n",
        "                                          is_split_into_words=True)\n",
        "    ids = encoding['input_ids']\n",
        "    mask = encoding['attention_mask']\n",
        "\n",
        "    extra_features = extract_features(headline, original_word, edit_word)\n",
        "\n",
        "    return {\n",
        "      'ids': torch.tensor(ids),\n",
        "      'mask': torch.tensor(mask),\n",
        "      'target': torch.tensor(target),\n",
        "      'extra_features': torch.tensor(extra_features)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWaxTh2UlDoy"
      },
      "source": [
        "class BertRegressionModel(nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super(BertRegressionModel, self).__init__()\n",
        "\n",
        "    if model_name == 'distilbert':\n",
        "      self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    elif model_name == 'roberta':\n",
        "      self.bert = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    self.num_extra_features = EXTRA_FEATURES\n",
        "    self.linear1 = nn.Linear(768, 32)\n",
        "    self.linear2 = nn.Linear(32 + self.num_extra_features, 1)\n",
        "\n",
        "  def forward(self, ids, mask, extra_features):\n",
        "    embeddings = self.bert(input_ids=ids, attention_mask=mask)[0]\n",
        "    x = self.linear1(embeddings[:, 0])\n",
        "    concat_features = torch.cat([x, extra_features], dim=1)\n",
        "    x = self.linear2(concat_features)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g190qtchuI2W"
      },
      "source": [
        "### Approach 1: Using pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp9d32sOlDo6",
        "scrolled": true
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Initialise chosen BERT model and tokenizer\n",
        "# To change the model to use Roberta, pass in 'roberta' to the regression model\n",
        "# and uncomment the tokenizer below.\n",
        "model = BertRegressionModel('distilbert').to(DEVICE)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "training_data = [train_df['edited'], train_df['originalWord'], train_df['edit']]\n",
        "training_labels = train_df['meanGrade']\n",
        "test_data = test_df['edited']\n",
        "train_and_dev = Task1Dataset(tokenizer, training_data, training_labels)\n",
        "\n",
        "# Split training data into train and dev sets\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxhZKMC783t"
      },
      "source": [
        "# Do the training\n",
        "distil_model = BertRegressionModel('distilbert').to(DEVICE)\n",
        "optimizer_distil = torch.optim.Adam(distil_model.parameters())\n",
        "tokenizer_distil = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_model = BertRegressionModel('roberta').to(DEVICE)\n",
        "optimizer_roberta = torch.optim.Adam(roberta_model.parameters())\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "print('Training roberta model...')\n",
        "performance_stats_roberta = train(train_loader, dev_loader, roberta_model, epochs, optimizer_roberta)\n",
        "print('Training distil-bert model...')\n",
        "performance_stats_distil = train(train_loader, dev_loader, distil_model, epochs, optimizer_distil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQ1bqH13ZXx"
      },
      "source": [
        "# Save models\n",
        "torch.save(roberta_model, 'roberta-model.pt')\n",
        "torch.save(distil_model, 'distilbert-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_YyhDlPzSaP"
      },
      "source": [
        "# Plot losses for both pre trained models\n",
        "epoch_losses_1, rmse_1 = zip(*performance_stats_distil)\n",
        "epoch_losses_2, rmse_2 = zip(*performance_stats_roberta)\n",
        "x = np.arange(epochs) + 1\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, epoch_losses_1, label='distil-bert')\n",
        "plt.plot(x, epoch_losses_2, label='roberta')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Pre-trained model losses')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, rmse_1, label='distil-bert')\n",
        "plt.plot(x, rmse_2, label='roberta')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('Pre-trained model RMSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhJnAAIBZkMd"
      },
      "source": [
        "**Approach 1: Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOKk_2nIZjij"
      },
      "source": [
        "def comp_eval():\n",
        "  model = roberta_model\n",
        "  # Evaluate model on the competition data\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "  # Set training and test data\n",
        "  comp_ids = comp_df['id']\n",
        "  comp_data = [comp_df['edited'], comp_df['originalWord'], comp_df['edit']]\n",
        "  comp_labels = comp_df['meanGrade']\n",
        "\n",
        "  comp_data = Task1Dataset(tokenizer, comp_data, comp_labels)\n",
        "  comp_loader = torch.utils.data.DataLoader(comp_data, batch_size=32)\n",
        "\n",
        "  _,_,predictions,_ = eval(comp_loader, model)\n",
        "\n",
        "  comp_preds = {'id': comp_ids, 'pred': predictions}\n",
        "  comp_preds = pd.DataFrame(comp_preds)\n",
        "  comp_preds.to_csv('pred_outputs.csv', index=False)\n",
        "  print('Saved pred_outputs.csv\\n')\n",
        "\n",
        "  min_score = min(predictions)\n",
        "  max_score = max(predictions)\n",
        "  mean_score = sum(predictions) / len(predictions)\n",
        "  range = max_score - min_score\n",
        "  print(f'Min predicted score: {min_score}, Max predicted score: {max_score}, Range: {range}, Mean Predicted Score: {mean_score}\\n')  \n",
        "  print(f'MSE and RMSE on the Competition Test Set:')\n",
        "  output = model_performance(predictions, comp_labels, print_output=True)\n",
        "comp_eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dID3NDP3cxwn"
      },
      "source": [
        "## Approach 2\n",
        "No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gm47T4lDpQ"
      },
      "source": [
        "# TODO: comment on what this is\n",
        "\n",
        "train_and_dev = train_df['edited']\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edited'], train_df['meanGrade'],\n",
        "                                                                        test_size=(0.2),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "print(len(training_data))\n",
        "\n",
        "# We train a Tf-idf model\n",
        "#count_vect = CountVectorizer(stop_words='english')\n",
        "#count_vect = CountVectorizer(stop_words='english', ngram_range=(2,2))\n",
        "count_vect = CountVectorizer(stop_words='english', ngram_range=(3,3))\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "#transformer = TfidfTransformer().fit(train_counts)\n",
        "#train_counts = transformer.transform(train_counts)\n",
        "#regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "regression_model = Ridge().fit(train_counts, training_y)\n",
        "\n",
        "train_sizes=[1, 100, 500, 1000, 2000, 5000, 6176]\n",
        "\n",
        "train_sizes, train_scores, validation_scores = learning_curve(\n",
        "    estimator = LinearRegression(),\n",
        "    X = train_counts,\n",
        "    y = training_y, train_sizes = train_sizes, cv = 5,\n",
        "    scoring = 'neg_mean_squared_error')\n",
        "\n",
        "train_scores_mean = -train_scores.mean(axis = 1)\n",
        "validation_scores_mean = -validation_scores.mean(axis = 1)\n",
        "print(train_scores)\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('MSE', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.legend()\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "#transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "#test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyHwHkUlDpa"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DA3q4o1lDpd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhxU3EJ8vDFe"
      },
      "source": [
        "### Neural Network Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nOS3S4fisID"
      },
      "source": [
        "# Preprocessing steps for recognising entities, building vocab\n",
        "\n",
        "# Tokenize with Spacy Named Entity Recognition\n",
        "ENTITY_TOKEN_PATTERN = re.compile('<.+?>|\\w+')\n",
        "def tokenize_headline_with_ner(headline):\n",
        "  # Entity recognition - wrap entities in angle brackets\n",
        "  doc = ner(headline)\n",
        "  for e in doc.ents:\n",
        "    headline = re.sub(e.text, f\"<{e.text}>\", headline)\n",
        "\n",
        "  # Tokenize headline (respecting detected entities)\n",
        "  headline = [t.lower() for t in ENTITY_TOKEN_PATTERN.findall(headline)]\n",
        "\n",
        "  return headline\n",
        "\n",
        "# Create training vocab, with dedicated words for Spacy recognised entities\n",
        "def create_vocab_with_ner(data, stopwords):\n",
        "  vocabulary = []\n",
        "  max_len = 0\n",
        "  for headline in data:\n",
        "    headline = tokenize_headline_with_ner(headline)\n",
        "\n",
        "    # Keep track of longest headline\n",
        "    if len(headline) > max_len:\n",
        "      max_len = len(headline)\n",
        "    \n",
        "    # Add new tokens to vocab\n",
        "    for token in headline:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "  # Convert vocab list to dict\n",
        "  w2i = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  w2i['<pad>'] = 0\n",
        "\n",
        "  return w2i, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmWVpk7yvHHZ"
      },
      "source": [
        "class Task1DatasetNN(Dataset):\n",
        "\n",
        "  def __init__(self, train_data, labels, max_len, word2idx):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "    self.max_len = max_len\n",
        "    self.word2idx = word2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    headline = self.x_train[item]\n",
        "\n",
        "    # Get extra features before processing\n",
        "    extra_features = torch.FloatTensor(extract_features_two(headline))\n",
        "\n",
        "    # Get tokenized headline, map to words/entities in vocab\n",
        "    headline = tokenize_headline_with_ner(headline)\n",
        "    vectorized_headline = [self.word2idx[t] for t in headline if t in self.word2idx]\n",
        "\n",
        "    # Build headline tensor with 0 padding if needed\n",
        "    headline_tensor = torch.zeros((self.max_len,)).long()\n",
        "    headline_tensor[:len(vectorized_headline)] = torch.LongTensor(vectorized_headline[:self.max_len])\n",
        "\n",
        "    target_tensor = torch.FloatTensor([self.y_train[item]])\n",
        "\n",
        "    return {\n",
        "      'encoding': headline_tensor,\n",
        "      'extra_features': extra_features,\n",
        "      'target': target_tensor\n",
        "    }"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AQh88fGyTob"
      },
      "source": [
        "#### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO8ntsyraNhp"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, num_extra_features):\n",
        "    super(FFNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    self.fc1 = nn.Linear(embedding_dim + num_extra_features, hidden_dim)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "  # Average sentence embedding (ignoring padding)\n",
        "  # Maps embedded : (batch_size, max_sent_len, embedding_dim) => (batch_size, embeddind_dim)\n",
        "  def average_embedding(self, embedded):\n",
        "    total_emb = torch.sum(embedded, dim=1)\n",
        "    non_zero_embs = torch.sum(embedded != 0, dim=[1])\n",
        "    return total_emb / non_zero_embs\n",
        "\n",
        "  def forward(self, x, extra_features):\n",
        "    # x has shape (batch_size, max_sent_len)\n",
        "    # extra_features has shape (batch_size, num_extra_features)\n",
        "\n",
        "    # Learn embedding\n",
        "    embedded = self.embedding(x)\n",
        "    averaged = self.average_embedding(embedded)\n",
        "\n",
        "    # Concat extra features\n",
        "    out = torch.cat([averaged, extra_features], dim=1)\n",
        "    \n",
        "    # Hidden and output layers\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.fc2(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1bRWS3D5qDz"
      },
      "source": [
        "# Train model\n",
        "def train_nn(train_loader, model, loss_fn, optimizer):\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "  performance_stats = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0 \n",
        "\n",
        "    for batch in train_loader:\n",
        "\n",
        "      encoding = batch['encoding'].to(DEVICE)\n",
        "      target = batch['target'].to(DEVICE).squeeze(1)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(encoding, extra_features).squeeze(1)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = loss_fn(predictions, target)\n",
        "      train_loss = loss.item()\n",
        "\n",
        "      # sse stats\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += train_loss * target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval_nn(dev_loader, model, loss_fn)\n",
        "\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "    performance_stats.append((valid_loss, valid_mse**0.5))\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} | Train Loss (Single?): {train_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuqqyp15dhA"
      },
      "source": [
        "# Evaluate performance\n",
        "def eval_nn(data_iter, model, loss_fn):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "\n",
        "      encoding = batch['encoding'].to(DEVICE)\n",
        "      target = batch['target'].to(DEVICE).squeeze(1)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(encoding, extra_features).squeeze(1)\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # Get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EbAKoMU71rr"
      },
      "source": [
        "EPOCHS = 10\n",
        "LRATE = 0.01\n",
        "EMBEDDING_DIM = 10 # ~4th root of vocab size\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "BATCH_SIZE = 50\n",
        "train_proportion = 0.8\n",
        "\n",
        "# Set training data\n",
        "training_data = train_df['edited']\n",
        "training_labels = train_df['meanGrade']\n",
        "\n",
        "# Build vocab with NER\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "w2i, max_headline_len = create_vocab_with_ner(training_data, stopwords)\n",
        "\n",
        "train_and_dev = Task1DatasetNN(training_data, training_labels, max_headline_len, w2i)\n",
        "\n",
        "# Split training data into train and dev sets\n",
        "train_examples = round(len(training_data)*train_proportion)\n",
        "dev_examples = len(training_data) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFFULGPJaRJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3257dc13-f166-4d9c-a910-e99ec6605f3a"
      },
      "source": [
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, vocab_size=len(w2i), num_extra_features=EXTRA_FEATURES_TWO)\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# Use MSE loss and the Adam optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LRATE)\n",
        "\n",
        "train_nn(train_loader, model, loss_fn, optimizer)\n",
        "eval_nn(dev_loader, model, loss_fn)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (embedding): Embedding(12077, 10, padding_idx=0)\n",
            "  (fc1): Linear(in_features=13, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
            ")\n",
            "| Epoch: 00 | Train Loss: 0.55 | Train MSE: 0.55 | Train RMSE: 0.74 | Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.60 | Train Loss (Single?): 0.246\n",
            "| Epoch: 01 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 | Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.62 | Train Loss (Single?): 0.312\n",
            "| Epoch: 02 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 | Val. Loss: 0.40 | Val. MSE: 0.40 |  Val. RMSE: 0.63 | Train Loss (Single?): 0.353\n",
            "| Epoch: 03 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 | Val. Loss: 0.45 | Val. MSE: 0.45 |  Val. RMSE: 0.67 | Train Loss (Single?): 0.110\n",
            "| Epoch: 04 | Train Loss: 0.18 | Train MSE: 0.18 | Train RMSE: 0.42 | Val. Loss: 0.40 | Val. MSE: 0.40 |  Val. RMSE: 0.64 | Train Loss (Single?): 0.204\n",
            "| Epoch: 05 | Train Loss: 0.15 | Train MSE: 0.15 | Train RMSE: 0.38 | Val. Loss: 0.44 | Val. MSE: 0.44 |  Val. RMSE: 0.66 | Train Loss (Single?): 0.182\n",
            "| Epoch: 06 | Train Loss: 0.13 | Train MSE: 0.13 | Train RMSE: 0.36 | Val. Loss: 0.43 | Val. MSE: 0.43 |  Val. RMSE: 0.65 | Train Loss (Single?): 0.067\n",
            "| Epoch: 07 | Train Loss: 0.11 | Train MSE: 0.11 | Train RMSE: 0.33 | Val. Loss: 0.44 | Val. MSE: 0.44 |  Val. RMSE: 0.67 | Train Loss (Single?): 0.102\n",
            "| Epoch: 08 | Train Loss: 0.10 | Train MSE: 0.10 | Train RMSE: 0.32 | Val. Loss: 0.45 | Val. MSE: 0.45 |  Val. RMSE: 0.67 | Train Loss (Single?): 0.070\n",
            "| Epoch: 09 | Train Loss: 0.09 | Train MSE: 0.09 | Train RMSE: 0.31 | Val. Loss: 0.47 | Val. MSE: 0.47 |  Val. RMSE: 0.69 | Train Loss (Single?): 0.077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.474731114244214,\n",
              " 0.474731107820501,\n",
              " array([ 0.95104885,  1.0807132 , -0.22058079, ...,  1.2073691 ,\n",
              "         1.3676199 ,  0.7415808 ], dtype=float32),\n",
              " array([1.6, 2. , 0.6, ..., 0. , 0.8, 0. ], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AF76a9uZ7f"
      },
      "source": [
        "#### Approach 2: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UwmD0FVp2b2",
        "outputId": "c48f3128-c994-4a63-97ae-b6a8a3f295ea"
      },
      "source": [
        "def comp_eval2():\n",
        "  # Set training and test data\n",
        "  comp_ids = comp_df['id']\n",
        "  comp_labels = comp_df['meanGrade']\n",
        "\n",
        "  comp_data = Task1DatasetNN(comp_df['edited'], comp_labels, max_headline_len, w2i)\n",
        "  comp_loader = torch.utils.data.DataLoader(comp_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "  _,_,predictions,_ = eval_nn(comp_loader, model, loss_fn)\n",
        "\n",
        "  comp_preds = {'id': comp_ids, 'pred': predictions}\n",
        "  comp_preds = pd.DataFrame(comp_preds)\n",
        "  comp_preds.to_csv('pred_outputs_approach2.csv', index=False)\n",
        "  print('Saved pred_outputs2.csv\\n')\n",
        "  # predictions = [x if str(x) != \"nan\" else 0 for x in predictions]\n",
        "\n",
        "  min_score = min(predictions)\n",
        "  max_score = max(predictions)\n",
        "  mean_score = sum(predictions) / len(predictions)\n",
        "  range = max_score - min_score\n",
        "  print(f'Min predicted score: {min_score}, Max predicted score: {max_score}, Range: {range}, Mean Predicted Score: {mean_score}\\n')  \n",
        "  print(f'MSE and RMSE on the Competition Test Set:')\n",
        "  output = model_performance(predictions, comp_labels, print_output=True)\n",
        "comp_eval2()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved pred_outputs2.csv\n",
            "\n",
            "Min predicted score: -1.8207062482833862, Max predicted score: 4.194506645202637, Range: 6.0152130126953125, Mean Predicted Score: nan\n",
            "\n",
            "MSE and RMSE on the Competition Test Set:\n",
            "| MSE: 0.59 | RMSE: 0.77 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}