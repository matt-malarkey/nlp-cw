{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "task_1_main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvyemfDlDmu"
      },
      "source": [
        "# NLP CW Task 1\n",
        "\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWFk-kelDoA"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "import torchtext\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binned_statistic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import codecs\n",
        "import tqdm\n",
        "import re"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lJCEYZNOleKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14eb9a06-c210-4ef2-ba59-da01ae4d435f"
      },
      "source": [
        "# Get pretrained GloVe embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
        "\n",
        "# Install packages that Colab does not provide automatically\n",
        "!pip -q install transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, BertPreTrainedModel, DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get test and train data files\n",
        "if not os.path.exists('dev.csv'):\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/dev.csv\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/train.csv"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09jt8VRlDoM"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqhlzLl6lDoO"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('dev.csv')"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RCmF7xulDoP"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAgZW6K1lDoR"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "  \"\"\"\n",
        "  Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Training model.\")\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(device)\n",
        "\n",
        "  for epoch in range(1, number_epoch+1):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0  # Observations used for training so far\n",
        "\n",
        "    for batch in train_iter:\n",
        "      ids = batch['ids'].to(device).squeeze()\n",
        "      mask = batch['mask'].to(device).squeeze()\n",
        "      target = batch['target'].to(device, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(device)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "    Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXeDgHmlDob"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "  \"\"\"\n",
        "  Evaluating model performance on the dev set\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      ids = batch['ids'].to(device).squeeze()\n",
        "      mask = batch['mask'].to(device).squeeze()\n",
        "      target = batch['target'].to(device, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(device)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # We get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_22fHHElDog"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "  \"\"\"\n",
        "  Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "  \"\"\"\n",
        "  sq_error = (output - target)**2\n",
        "\n",
        "  sse = np.sum(sq_error)\n",
        "  mse = np.mean(sq_error)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  if print_output:\n",
        "    print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "  return sse, mse"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyew1QfTSuzM"
      },
      "source": [
        "# Data Analysis\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhQ12OmSzD6"
      },
      "source": [
        "# Basic Stats\r\n",
        "num_headlines = len(train_df)\r\n",
        "max_len = max(train_df['original'].apply(len))\r\n",
        "avg_score = train_df['meanGrade'].mean()\r\n",
        "print(f'Number of Headlines: {num_headlines}, Max Headline Length: {max_len}, Avg Score: {avg_score:.2f}')\r\n",
        "\r\n",
        "# Most common words\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "most_common_words = Counter(\" \".join(train_df['original']).split()).most_common(100)\r\n",
        "stopwords.append(string.punctuation)\r\n",
        "most_common_words = [(word, count) for (word, count) in most_common_words if word not in stopwords and word.isalpha()][:5]\r\n",
        "print(f'Most common words: {most_common_words}')\r\n",
        "\r\n",
        "# Analysis of Trump Headlines\r\n",
        "trump_headlines = train_df[train_df['original'].str.lower().str.contains('trump')]\r\n",
        "num_headlines_with_trump = len(trump_headlines)\r\n",
        "trump_avg_score = trump_headlines['meanGrade'].mean()\r\n",
        "prob_headline_contains_trump = num_headlines_with_trump / num_headlines\r\n",
        "trump_value_counts = trump_headlines['meanGrade'].value_counts(bins=3)\r\n",
        "trump_proportion_gt_one = (trump_value_counts[2] + trump_value_counts[3]) * 100 / num_headlines_with_trump\r\n",
        "print(f'Headlines with Trump: {prob_headline_contains_trump * 100:.2f}%, Avg Score: {trump_avg_score:.2f}')\r\n",
        "print(f'Proportion of Trump Headlines with score > 1: {trump_proportion_gt_one:.2f}%')\r\n",
        "\r\n",
        "# Analysis of word similarity and length of headlines\r\n",
        "similarity_data = []\r\n",
        "headline_length_data = []\r\n",
        "for row in train_df.itertuples():\r\n",
        "  headline_length_data.append((len(row.original.split(' ')), row.meanGrade))\r\n",
        "  for word in row.original.split(' '):\r\n",
        "    if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\r\n",
        "      original_word_vec = glove[word[1:-2].lower()].unsqueeze(0)\r\n",
        "      edited_word_vec = glove[row.edit.lower()].unsqueeze(0)\r\n",
        "      distance = torch.cosine_similarity(original_word_vec, edited_word_vec).item() \r\n",
        "      similarity_data.append((distance, row.meanGrade))\r\n",
        "\r\n",
        "# Plot word similarity vs meanGrade\r\n",
        "plt.figure(figsize=(10, 5))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "mean_stat_similarity = binned_statistic(*zip(*similarity_data), bins=6, range=(-0.5, 1.0))\r\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\r\n",
        "plt.xlabel('Cosine Distance')\r\n",
        "plt.ylabel('Avg Score')\r\n",
        "plt.title('Word similarity vs meanGrade')\r\n",
        "\r\n",
        "\r\n",
        "# Plot headline length vs meanGrade\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "mean_stat_headline_len = binned_statistic(*zip(*headline_length_data), bins=30, range=(0, 30))\r\n",
        "plt.plot(mean_stat_headline_len.bin_edges[:-1], mean_stat_headline_len.statistic)\r\n",
        "plt.xlabel('Number of Words')\r\n",
        "plt.ylabel('Avg Score')\r\n",
        "plt.title('Headline Length vs meanGrade')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cenE48y9leKo"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2spyhNrBleKo"
      },
      "source": [
        "# Pattern to match words and multiword tags\n",
        "# See https://regex101.com/r/a2cWZL/1 for example\n",
        "# TOKEN_PATTERN = re.compile(\"[A-Za-z#]+|<[A-Za-z#]+\\s[A-Za-z]+\\/>\")\n",
        "\n",
        "# Patten to match only words\n",
        "TOKEN_PATTERN = re.compile(\"\\w+\")\n",
        "\n",
        "# TODO: processing of:\n",
        "# - hashtags,\n",
        "# - words with a \"-\" in between them\n",
        "# - punctuation\n",
        "# - entity names\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return TOKEN_PATTERN.findall(sentence)\n",
        "\n",
        "# Remove angle brackets and lowercase\n",
        "def preprocess(word):\n",
        "  if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "    word = word[1:-2]\n",
        "  return word.lower()\n",
        "\n",
        "def create_vocab(data):\n",
        "  \"\"\"\n",
        "  Creating a corpus of all the tokens used\n",
        "  \"\"\"\n",
        "  # Let us put the tokenized corpus in a list\n",
        "  tokenized_corpus = [[preprocess(t) for t in tokenize(s)] for s in data]\n",
        "\n",
        "  # Create single list of all vocabulary\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "  return vocabulary, tokenized_corpus\n",
        "\n",
        "# Extra features:\n",
        "# [0] - binary if 'Trump' in headline\n",
        "# TODO ...\n",
        "def extract_features(headline):\n",
        "  extra_features = [0]\n",
        "  if 'Trump' in headline:\n",
        "    extra_features[0] = 1\n",
        "  return extra_features"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aoC2hQrNleKp"
      },
      "source": [
        "# Inherits from Dataset class so the DataLoader can use it\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, tokenizer, train_data, labels):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    # Preprocess headline before embedding\n",
        "    headline = self.x_train[item]\n",
        "    processed_headline = ' '.join([preprocess(t) for t in tokenize(headline)])\n",
        "\n",
        "    target = self.y_train[item]\n",
        "    encoding = self.tokenizer.encode_plus(processed_headline, return_tensors='pt',\n",
        "                                          padding='max_length', truncation=True,\n",
        "                                          max_length=128, pad_to_max_length=True)\n",
        "    ids = encoding['input_ids']\n",
        "    mask = encoding['attention_mask']\n",
        "\n",
        "    extra_features = extract_features(headline)\n",
        "\n",
        "    return {\n",
        "      'ids': torch.tensor(ids),\n",
        "      'mask': torch.tensor(mask),\n",
        "      'target': torch.tensor(target),\n",
        "      'extra_features': torch.tensor(extra_features)\n",
        "    }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWaxTh2UlDoy"
      },
      "source": [
        "class BertRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BertRegressionModel, self).__init__()\n",
        "\n",
        "    self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    # self.bert = RobertaModel.from_pretrained('roberta-base')\n",
        "    self.num_extra_features = 1\n",
        "    self.linear1 = nn.Linear(768, 32)\n",
        "    self.linear2 = nn.Linear(32 + self.num_extra_features, 1)\n",
        "\n",
        "  def forward(self, ids, mask, extra_features):\n",
        "    embeddings = self.bert(input_ids=ids, attention_mask=mask)[0]\n",
        "    x = self.linear1(embeddings[:, 0])\n",
        "    concat_features = torch.cat([x, extra_features], dim=1)\n",
        "    x = self.linear2(concat_features)\n",
        "    return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g190qtchuI2W"
      },
      "source": [
        "### Approach 1: Usiung pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bp9d32sOlDo6"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Initialise chosen BERT model and tokenizer\n",
        "# To change the model to use Roberta, uncomment the bert model in the cell above\n",
        "# and uncomment the tokenizer below.\n",
        "model = BertRegressionModel().to(device)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Set training and test data\n",
        "training_data = train_df['original']\n",
        "training_labels = train_df['meanGrade']\n",
        "test_data = test_df['original']\n",
        "train_and_dev = Task1Dataset(tokenizer, training_data, training_labels)\n",
        "\n",
        "# Split training data into train and dev sets\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxhZKMC783t"
      },
      "source": [
        "# Do the training\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train(train_loader, dev_loader, model, epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdeFaoc3lDpK"
      },
      "source": [
        "### Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gm47T4lDpQ"
      },
      "source": [
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyHwHkUlDpa"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DA3q4o1lDpd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}