{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "task_1_main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvyemfDlDmu"
      },
      "source": [
        "# NLP CW Task 1\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWFk-kelDoA"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "import torchtext\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binned_statistic\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk6g6430Lsen"
      },
      "source": [
        "# Get pretrained GloVe embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lJCEYZNOleKm"
      },
      "source": [
        "# Install packages that Colab does not provide automatically\n",
        "!pip -q install transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel, BertPreTrainedModel, DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download spacy model\n",
        "!spacy download en_core_web_sm -q\n",
        "ner = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Get test and train data files\n",
        "if not os.path.exists('dev.csv'):\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/dev.csv\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/train.csv\n",
        "  !wget -q --show-progress https://raw.githubusercontent.com/matt-malarkey/nlp-cw-data/master/competition_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09jt8VRlDoM"
      },
      "source": [
        "# Set random seed and device\n",
        "SEED = 1\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqhlzLl6lDoO"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('dev.csv')\n",
        "comp_df = pd.read_csv('competition_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP5Yz343ddbL"
      },
      "source": [
        "# Build columns in dataset for edited headline and original word\n",
        "def build_edit_cols(dataset):\n",
        "  edited = []\n",
        "  original_words = []\n",
        "  for i in range(len(dataset)):\n",
        "    original_words.append(re.search(r'<(.*)/>', dataset['original'][i]).group(1))\n",
        "    edited.append(re.sub(r'<.*/>', dataset['edit'][i], dataset['original'][i]))\n",
        "  dataset['edited'] = edited\n",
        "  dataset['originalWord'] = original_words\n",
        "\n",
        "build_edit_cols(train_df)\n",
        "build_edit_cols(test_df)\n",
        "build_edit_cols(comp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8zZIRguoZYQ-"
      },
      "source": [
        "# Approach 1\n",
        "Using pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "inHh1HElZYQ_"
      },
      "source": [
        "# Training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, optimizer):\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  performance_stats = []\n",
        "\n",
        "  for epoch in range(1, number_epoch+1):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0  # Observations used for training so far\n",
        "\n",
        "    for batch in train_iter:\n",
        "      ids = batch['ids'].to(DEVICE).squeeze()\n",
        "      mask = batch['mask'].to(DEVICE).squeeze()\n",
        "      target = batch['target'].to(DEVICE, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "    performance_stats.append((valid_loss, valid_mse**0.5))\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "    Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
        "\n",
        "  return performance_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXeDgHmlDob"
      },
      "source": [
        "# Evaluate performance\n",
        "def eval(data_iter, model):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      ids = batch['ids'].to(DEVICE).squeeze()\n",
        "      mask = batch['mask'].to(DEVICE).squeeze()\n",
        "      target = batch['target'].to(DEVICE, dtype=torch.float)\n",
        "      extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(ids, mask, extra_features).squeeze(1)\n",
        "\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # We get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_22fHHElDog"
      },
      "source": [
        "# Get SSE and MSE\n",
        "def model_performance(output, target, print_output=False):\n",
        "  sq_error = (output - target)**2\n",
        "  sse = np.sum(sq_error)\n",
        "  mse = np.mean(sq_error)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  if print_output:\n",
        "    print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "  return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyew1QfTSuzM"
      },
      "source": [
        "# Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhQ12OmSzD6"
      },
      "source": [
        "# Basic Stats\n",
        "num_headlines = len(train_df)\n",
        "max_len = max(train_df['original'].apply(len))\n",
        "avg_score = train_df['meanGrade'].mean()\n",
        "print(f'Number of Headlines: {num_headlines}, Max Headline Length: {max_len}, Avg Score: {avg_score:.2f}')\n",
        "\n",
        "# Most common words\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "most_common_words = Counter(\" \".join(train_df['original']).split()).most_common(100)\n",
        "stopwords.append(string.punctuation)\n",
        "most_common_words = [(word, count) for (word, count) in most_common_words if word not in stopwords and word.isalpha()][:5]\n",
        "print(f'Most common words: {most_common_words}')\n",
        "\n",
        "# Analysis of Trump Headlines\n",
        "trump_headlines = train_df[train_df['original'].str.lower().str.contains('trump')]\n",
        "num_headlines_with_trump = len(trump_headlines)\n",
        "trump_avg_score = trump_headlines['meanGrade'].mean()\n",
        "prob_headline_contains_trump = num_headlines_with_trump / num_headlines\n",
        "trump_value_counts = trump_headlines['meanGrade'].value_counts(bins=3)\n",
        "trump_proportion_gt_one = (trump_value_counts[2] + trump_value_counts[3]) * 100 / num_headlines_with_trump\n",
        "print(f'Headlines with Trump: {prob_headline_contains_trump * 100:.2f}%, Avg Score: {trump_avg_score:.2f}')\n",
        "print(f'Proportion of Trump Headlines with score > 1: {trump_proportion_gt_one:.2f}%')\n",
        "\n",
        "# Analysis of word similarity and length of headlines\n",
        "semantic_similarity_data = []\n",
        "sentence_similarity_data = []\n",
        "levenshtein_similarity_data = []\n",
        "headline_length_data = []\n",
        "\n",
        "for row in train_df.itertuples():\n",
        "  all_words = row.original.split(' ')\n",
        "  headline_length_data.append((len(all_words), row.meanGrade))\n",
        "  sentence_vec = torch.zeros((1,50))\n",
        "  original_word_vec = glove[row.originalWord.lower()].unsqueeze(0)\n",
        "  edited_word_vec = glove[row.edit.lower()].unsqueeze(0)\n",
        "\n",
        "  for i, word in enumerate(all_words):\n",
        "    if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "      word = word[1:-2].lower()\n",
        "    sentence_vec += glove[word.lower()].unsqueeze(0)\n",
        "\n",
        "  semantic_distance = torch.cosine_similarity(original_word_vec, edited_word_vec).item()\n",
        "  levenshtein_distance = nltk.edit_distance(row.originalWord.lower(), row.edit.lower())\n",
        "  semantic_similarity_data.append((semantic_distance, row.meanGrade))\n",
        "  levenshtein_similarity_data.append((levenshtein_distance, row.meanGrade))\n",
        "  sentence_semantic_distance = torch.cosine_similarity(sentence_vec, edited_word_vec)\n",
        "  sentence_similarity_data.append((sentence_semantic_distance, row.meanGrade))\n",
        "\n",
        "\n",
        "# Analysis of # of clause separators\n",
        "separators_data = []\n",
        "for i in range(len(train_df['original'])):\n",
        "  matches = re.findall(\"[;,:(...)]\", train_df['original'][i])\n",
        "  separators_data.append((len(matches), train_df['meanGrade'][i]))\n",
        "\n",
        "# Plot word similarity vs meanGrade\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 3, 1)\n",
        "mean_stat_similarity = binned_statistic(*zip(*semantic_similarity_data), bins=6, range=(-0.5, 1.0))\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Cosine Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Semantic similarity vs meanGrade')\n",
        "\n",
        "# Levenshtein distance vs meanGrade\n",
        "plt.subplot(2, 3, 2)\n",
        "mean_stat_similarity = binned_statistic(*zip(*levenshtein_similarity_data), bins=6)\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Levenshtein Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Word spelling similarity vs meanGrade')\n",
        "\n",
        "# Semantic similarity of word to headline vs meanGrade\n",
        "plt.subplot(2, 3, 3)\n",
        "mean_stat_similarity = binned_statistic(*zip(*sentence_similarity_data), bins=6)\n",
        "plt.plot(mean_stat_similarity.bin_edges[:-1] + 0.125, mean_stat_similarity.statistic)\n",
        "plt.xlabel('Cosine Distance')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Semantic similarity of word to headline vs meanGrade')\n",
        "\n",
        "# Plot clause separator count vs meanGrade\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot()\n",
        "mean_stat_seps = binned_statistic(*zip(*separators_data), bins=6)\n",
        "plt.plot(mean_stat_seps.bin_edges[:-1], mean_stat_seps.statistic)\n",
        "plt.xlabel('Number of Clause Separators')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Clause Separator Count vs meanGrade')\n",
        "\n",
        "# Plot headline length vs meanGrade\n",
        "plt.subplot(2, 3, 5)\n",
        "mean_stat_headline_len = binned_statistic(*zip(*headline_length_data), bins=30, range=(0, 30))\n",
        "plt.plot(mean_stat_headline_len.bin_edges[:-1], mean_stat_headline_len.statistic)\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Avg Score')\n",
        "plt.title('Headline Length vs meanGrade')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cenE48y9leKo"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2spyhNrBleKo"
      },
      "source": [
        "# Pattern to match words and multiword tags\n",
        "# See https://regex101.com/r/a2cWZL/1 for example\n",
        "# TOKEN_PATTERN = re.compile(\"[A-Za-z#]+|<[A-Za-z#]+\\s[A-Za-z]+\\/>\")\n",
        "\n",
        "# Patten to match only words\n",
        "TOKEN_PATTERN = re.compile(\"\\w+\")\n",
        "CLAUSE_SEPS = re.compile(\"[;,:(...)]\")\n",
        "\n",
        "# TODO: processing of:\n",
        "# - hashtags,\n",
        "# - words with a \"-\" in between them\n",
        "# - punctuation\n",
        "# - entity names\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return TOKEN_PATTERN.findall(sentence)\n",
        "\n",
        "# Remove angle brackets and lowercase\n",
        "def preprocess(word):\n",
        "  if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "    word = word[1:-2]\n",
        "  return word.lower()\n",
        "\n",
        "def create_vocab(data):\n",
        "  \"\"\"\n",
        "  Creating a corpus of all the tokens used\n",
        "  \"\"\"\n",
        "  # Let us put the tokenized corpus in a list\n",
        "  tokenized_corpus = [[preprocess(t) for t in tokenize(s)] for s in data]\n",
        "\n",
        "  # Create single list of all vocabulary\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "  return vocabulary, tokenized_corpus\n",
        "\n",
        "# Extra features:\n",
        "# [0] - binary if 'Trump' in headline\n",
        "# [1] - length of headline\n",
        "# [2] - number of clause separators: elipsis, commas, colons or semi colons\n",
        "# [3] - semantic distance between original and edit word glove embedding\n",
        "# [4] - semantic distance between edit word and total original headline glove embedding\n",
        "# [5] - edit distance between edit word and original word\n",
        "EXTRA_FEATURES = 6\n",
        "def extract_features(raw_headline, original_word, edit_word):\n",
        "  sentence_vec = torch.zeros((1, 50))\n",
        "  original_word_vec = glove[original_word.lower()].unsqueeze(0)\n",
        "  edited_word_vec = glove[edit_word.lower()].unsqueeze(0)\n",
        "\n",
        "  for i, word in enumerate(raw_headline.split(' ')):\n",
        "    if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "      word = word[1:-2].lower()\n",
        "    sentence_vec += glove[word.lower()].unsqueeze(0)\n",
        "\n",
        "  semantic_distance = torch.cosine_similarity(original_word_vec, edited_word_vec).item()\n",
        "  levenshtein_distance = nltk.edit_distance(original_word.lower(), edit_word.lower())\n",
        "  sentence_semantic_distance = torch.cosine_similarity(sentence_vec, edited_word_vec)\n",
        "\n",
        "  return [\n",
        "    int('Trump' in raw_headline),\n",
        "    len(raw_headline),\n",
        "    len(CLAUSE_SEPS.findall(raw_headline)),\n",
        "    semantic_distance,\n",
        "    sentence_semantic_distance,\n",
        "    levenshtein_distance\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aoC2hQrNleKp"
      },
      "source": [
        "# Inherits from Dataset class so the DataLoader can use it\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, tokenizer, train_data, labels):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    # Preprocess headline before embedding\n",
        "    headline = self.x_train[0][item]\n",
        "    original_word = self.x_train[1][item]\n",
        "    edit_word = self.x_train[2][item]\n",
        "    # processed_headline = ' '.join([preprocess(t) for t in tokenize(headline)])\n",
        "    processed_headline = [preprocess(t) for t in tokenize(headline)]\n",
        "\n",
        "    target = self.y_train[item]\n",
        "\n",
        "    # Use is_split_into_words=True to allow for our own tokenizing\n",
        "    # - stops encode_plus splitting up unknown words into multiple parts\n",
        "    encoding = self.tokenizer.encode_plus(processed_headline, return_tensors='pt',\n",
        "                                          padding='max_length', truncation=True,\n",
        "                                          max_length=128, pad_to_max_length=True,\n",
        "                                          is_split_into_words=True)\n",
        "    ids = encoding['input_ids']\n",
        "    mask = encoding['attention_mask']\n",
        "\n",
        "    extra_features = extract_features(headline, original_word, edit_word)\n",
        "\n",
        "    return {\n",
        "      'ids': torch.tensor(ids),\n",
        "      'mask': torch.tensor(mask),\n",
        "      'target': torch.tensor(target),\n",
        "      'extra_features': torch.tensor(extra_features)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWaxTh2UlDoy"
      },
      "source": [
        "class BertRegressionModel(nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super(BertRegressionModel, self).__init__()\n",
        "\n",
        "    if model_name == 'distilbert':\n",
        "      self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    elif model_name == 'roberta':\n",
        "      self.bert = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    self.num_extra_features = EXTRA_FEATURES\n",
        "    self.linear1 = nn.Linear(768, 32)\n",
        "    self.linear2 = nn.Linear(32 + self.num_extra_features, 1)\n",
        "\n",
        "  def forward(self, ids, mask, extra_features):\n",
        "    embeddings = self.bert(input_ids=ids, attention_mask=mask)[0]\n",
        "    x = self.linear1(embeddings[:, 0])\n",
        "    concat_features = torch.cat([x, extra_features], dim=1)\n",
        "    x = self.linear2(concat_features)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g190qtchuI2W"
      },
      "source": [
        "### Approach 1: Using pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bp9d32sOlDo6"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Initialise chosen BERT model and tokenizer\n",
        "# To change the model to use Roberta, pass in 'roberta' to the regression model\n",
        "# and uncomment the tokenizer below.\n",
        "model = BertRegressionModel('distilbert').to(DEVICE)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "training_data = [train_df['edited'], train_df['originalWord'], train_df['edit']]\n",
        "training_labels = train_df['meanGrade']\n",
        "test_data = test_df['edited']\n",
        "train_and_dev = Task1Dataset(tokenizer, training_data, training_labels)\n",
        "\n",
        "# Split training data into train and dev sets\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxhZKMC783t"
      },
      "source": [
        "# Do the training\n",
        "distil_model = BertRegressionModel('distilbert').to(DEVICE)\n",
        "optimizer_distil = torch.optim.Adam(distil_model.parameters())\n",
        "tokenizer_distil = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_model = BertRegressionModel('roberta').to(DEVICE)\n",
        "optimizer_roberta = torch.optim.Adam(roberta_model.parameters())\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "print('Training roberta model...')\n",
        "performance_stats_roberta = train(train_loader, dev_loader, roberta_model, epochs, optimizer_roberta)\n",
        "print('Training distil-bert model...')\n",
        "performance_stats_distil = train(train_loader, dev_loader, distil_model, epochs, optimizer_distil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQ1bqH13ZXx"
      },
      "source": [
        "# Save models\n",
        "torch.save(roberta_model, 'roberta-model.pt')\n",
        "torch.save(distil_model, 'distilbert-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_YyhDlPzSaP"
      },
      "source": [
        "# Plot losses for both pre trained models\n",
        "epoch_losses_1, rmse_1 = zip(*performance_stats_distil)\n",
        "epoch_losses_2, rmse_2 = zip(*performance_stats_roberta)\n",
        "x = np.arange(epochs) + 1\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, epoch_losses_1, label='distil-bert')\n",
        "plt.plot(x, epoch_losses_2, label='roberta')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Pre-trained model losses')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, rmse_1, label='distil-bert')\n",
        "plt.plot(x, rmse_2, label='roberta')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('Pre-trained model RMSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhJnAAIBZkMd"
      },
      "source": [
        "**Approach 1: Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOKk_2nIZjij"
      },
      "source": [
        "model = roberta_model\r\n",
        "# Evaluate model on the competition data\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "\r\n",
        "# Set training and test data\r\n",
        "comp_ids = comp_df['id']\r\n",
        "comp_data = [comp_df['edited'], comp_df['originalWord'], comp_df['edit']]\r\n",
        "comp_labels = comp_df['meanGrade']\r\n",
        "\r\n",
        "comp_data = Task1Dataset(tokenizer, comp_data, comp_labels)\r\n",
        "comp_loader = torch.utils.data.DataLoader(comp_data, batch_size=32)\r\n",
        "\r\n",
        "_,_,predictions,_ = eval(comp_loader, model)\r\n",
        "\r\n",
        "comp_preds = {'id': comp_ids, 'pred': predictions}\r\n",
        "comp_preds = pd.DataFrame(comp_preds)\r\n",
        "comp_preds.to_csv('pred_outputs.csv', index=False)\r\n",
        "print('Saved pred_outputs.csv\\n')\r\n",
        "\r\n",
        "min_score = min(predictions)\r\n",
        "max_score = max(predictions)\r\n",
        "mean_score = sum(predictions) / len(predictions)\r\n",
        "range = max_score - min_score\r\n",
        "print(f'Min predicted score: {min_score}, Max predicted score: {max_score}, Range: {range}, Mean Predicted Score: {mean_score}\\n')  \r\n",
        "print(f'MSE and RMSE on the Competition Test Set:')\r\n",
        "output = model_performance(predictions, comp_labels, print_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dID3NDP3cxwn"
      },
      "source": [
        "## Approach 2\n",
        "No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2CvKSNGLLPO"
      },
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([doc])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['Word', 'Count']\n",
        "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
        "count_occur_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gm47T4lDpQ"
      },
      "source": [
        "# TODO:\n",
        "# - remove stop words\n",
        "# - different norms\n",
        "# - pre: weigh query terms higher for certain\n",
        "\n",
        "\n",
        "\n",
        "####### TODO: is this up to date?? why is it only training on the edit words??\n",
        "#######  need to use train_df['edited'] to train on the edited headline\n",
        "\n",
        "\n",
        "\n",
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyHwHkUlDpa"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DA3q4o1lDpd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhxU3EJ8vDFe"
      },
      "source": [
        "### Approach 2: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmWVpk7yvHHZ"
      },
      "source": [
        "class Task1DatasetNN(Dataset):\n",
        "\n",
        "  def __init__(self, train_data, labels, max_len, word2idx, entity2idx, stopwords=nltk.corpus.stopwords.words('english')):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "    self.stopwords = stopwords\n",
        "    self.max_len = max_len\n",
        "    self.word2idx = word2idx\n",
        "    self.entity2idx = entity2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    headline = self.x_train[item]\n",
        "\n",
        "    # Remove punctuation\n",
        "    headline = re.sub(r'[^\\w\\s]', '', headline)\n",
        "\n",
        "    # Map headlines to list of entity IDs\n",
        "    # entities = [self.entity2idx[str(e)] for e in ner(headline).ents if str(e) in self.entity2idx]\n",
        "\n",
        "    # Tokenize, lowercase, remove stop words\n",
        "    headline = [w.lower() for w in headline.split() if w not in self.stopwords and len(w)]\n",
        "\n",
        "    # Get headline vector\n",
        "    vectorized_headline = [self.word2idx[tok] for tok in headline if tok in self.word2idx]\n",
        "    headline_tensor = torch.zeros((self.max_len,)).long()\n",
        "    headline_tensor[:len(vectorized_headline)] = torch.LongTensor(vectorized_headline)\n",
        "\n",
        "    target_tensor = torch.FloatTensor([self.y_train[item]])\n",
        "    extra_features = []\n",
        "\n",
        "    return {\n",
        "      'encoding': headline_tensor,\n",
        "      'extra_features': torch.tensor(extra_features),\n",
        "      'target': target_tensor\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThIL2nlIaAlS"
      },
      "source": [
        "TOKEN_PATTERN = re.compile(\"\\w+\")\n",
        "def tokenize(sentence):\n",
        "  return TOKEN_PATTERN.findall(sentence)\n",
        "\n",
        "# Remove angle brackets and lowercase\n",
        "def preprocess(word):\n",
        "  if len(word) > 2 and word[0] == '<' and word[-2] == '/' and word[-1] == '>' :\n",
        "    word = word[1:-2]\n",
        "  return word.lower()\n",
        "\n",
        "# Create vocab of all words\n",
        "def create_vocab(data):\n",
        "  # Let us put the tokenized corpus in a list\n",
        "  tokenized_corpus = [[preprocess(t) for t in tokenize(s)] for s in data]\n",
        "\n",
        "  # Create single list of all vocabulary\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "  return vocabulary, tokenized_corpus\n",
        "\n",
        "# TODO: need to tie this in with NER\n",
        "def build_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "  w2i = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  w2i['<pad>'] = 0\n",
        "  return w2i\n",
        "\n",
        "# Get NER lookup table from training data\n",
        "def build_entity2idx(corpus):\n",
        "  entities = {}\n",
        "  idx = 0\n",
        "  for s in corpus:\n",
        "    doc = ner(s)\n",
        "    for e in doc.ents:\n",
        "      e = str(e)\n",
        "      if e not in entities:\n",
        "        entities[e] = idx\n",
        "        idx+=1\n",
        "  return entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO8ntsyraNhp"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):\n",
        "    super(FFNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    # self.relu2 = nn.ReLU()\n",
        "\n",
        "  # Average sentence embedding (ignoring padding)\n",
        "  # Maps embedded : (batch_size, max_sent_len, embedding_dim) => (batch_size, embeddind_dim)\n",
        "  def average_embedding(self, embedded):\n",
        "    total_emb = torch.sum(embedded, dim=1)\n",
        "    non_zero_embs = torch.sum(embedded != 0, dim=[1])\n",
        "    return total_emb / non_zero_embs\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x has shape (batch_size, max_sent_len)\n",
        "    embedded = self.embedding(x)\n",
        "\n",
        "    # embedding has shape (batch_size, max_sent_len, embedding_dim)\n",
        "    # Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "    averaged = self.average_embedding(embedded)\n",
        "\n",
        "    # TODO: extra features\n",
        "\n",
        "    out = self.fc1(averaged)\n",
        "    out = self.relu1(out)\n",
        "    out = self.fc2(out)\n",
        "    # out = self.relu2(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1bRWS3D5qDz"
      },
      "source": [
        "# Train model\n",
        "def train_nn(train_loader, model):\n",
        "  # TODO <<<\n",
        "  # Recall that we did not apply any activation to our output layer, hence we need\n",
        "  # to make our outputs look like probabilities.\n",
        "  # <<<<<<<<\n",
        "  loss_fn = nn.MSELoss().to(DEVICE)\n",
        "\n",
        "  performance_stats = []\n",
        "\n",
        "  # Start training\n",
        "  for epoch in range(EPOCHS):\n",
        "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "    # good practice to include even if do not use here\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0  # Observations used for training so far\n",
        "\n",
        "    for batch in train_loader:\n",
        "\n",
        "      encoding = batch['encoding'].to(DEVICE)\n",
        "      target = batch['target'].to(DEVICE).squeeze(1)\n",
        "      # extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      # we zero the gradients as they are not removed automatically\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "      # and we need to remove the dimension of size 1\n",
        "      predictions = model(encoding).squeeze(1)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = loss_fn(predictions, target)\n",
        "      train_loss = loss.item()\n",
        "\n",
        "      # sse stats\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "      # calculate the gradient of each parameter\n",
        "      loss.backward()\n",
        "\n",
        "      # update the parameters using the gradients and optimizer algorithm\n",
        "      optimizer.step()\n",
        "      \n",
        "      epoch_loss += train_loss * target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval(dev_loader, model)\n",
        "\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "    performance_stats.append((valid_loss, valid_mse**0.5))\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} | Train Loss (Single?): {train_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuqqyp15dhA"
      },
      "source": [
        "# Evaluate performance\n",
        "def eval_nn(data_iter, model):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_fn = loss_fn.to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "\n",
        "      encoding = batch['encoding'].to(DEVICE)\n",
        "      target = batch['target'].to(DEVICE).squeeze(1)\n",
        "      # extra_features = batch['extra_features'].to(DEVICE)\n",
        "\n",
        "      predictions = model(encoding).squeeze(1)\n",
        "\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # We get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EbAKoMU71rr"
      },
      "source": [
        "EPOCHS = 50\n",
        "LRATE = 0.5\n",
        "EMBEDDING_DIM = 10 # ~4th root of vocab size\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# Set training and test data\n",
        "training_data = train_df['edited']\n",
        "training_labels = train_df['meanGrade']\n",
        "test_data = test_df['edited']\n",
        "\n",
        "_vocab, tokenized_corpus = create_vocab(training_data)\n",
        "max_headline_len = max([len([w for w in s.split()]) for s in training_data])\n",
        "w2i = build_word2idx(tokenized_corpus)\n",
        "# e2i = build_entity2idx(training_data)\n",
        "e2i = {}\n",
        "\n",
        "train_and_dev = Task1DatasetNN(training_data, training_labels, max_headline_len, w2i, e2i)\n",
        "\n",
        "# TODO: what processing should be done on both train and dev and what should be done on just train???\n",
        "# Split training data into train and dev sets\n",
        "train_examples = round(len(training_data)*train_proportion)\n",
        "dev_examples = len(training_data) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev, (train_examples, dev_examples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFFULGPJaRJL"
      },
      "source": [
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(w2i), OUTPUT_DIM).to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "train_nn(train_loader, model)\n",
        "eval_nn(dev_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}